---
title: "Диплом"
output: html_notebook
---

# Анализ параметров в сложных распределениях

## Рассеяние

```{r}
e_culc <- function(samp){
  var(samp) * (length(samp) - 1) / length(samp) / mean(samp)
}
```

## Логарифмическое распределение

Табличная функция распредления логарифмического распределения:

```{r}
log_prepering <- function(p = 0.5){
  tdistr <- (-p) / log(1 - p)
  sum_distr <- tdistr
  sum <- c(sum_distr)
  k <- 2
  
  while(tdistr > 1e-10){
    tdistr <- tdistr * p / k * (k - 1)
    sum_distr <- sum_distr + tdistr
    sum <- c(sum, sum_distr)
    k <- k + 1
  }
  
  sum
}
```

Моделирование логарифмического распределения по табличной функции распределения:

```{r}
rlog <- function(sum){
  x <- runif(1)
  j <- 1
  
  while (x > sum[j]){
    j <- j + 1
  }
  
  j
}
```

Моделирование выборки логарифмического распределения:

```{r}
rnlog <- function(n = 1, sum){
  v <- replicate(n, rlog(sum))
  
  if(n == 0)
    v <- 0;
  
  v
}
```

Смоделируем выборку и построим гистограмму:

```{r}
v <- rnlog(10000, log_prepering(0.75))
hist(v[v < 9], probability = TRUE, breaks = seq(0.5, max(v) + 0.5, 1), xlim = c(0.5, 8.5), xlab = "Sample", main = "")
```

### Логарифмическое распределение с нулевым значением случайной величины

Заведём вероятность в нуле:

```{r}
p_0 <- 0.2
```

Построим таблицу вероятносте для нашего распределения:

```{r}
log_0_prepering <- function(p = 0.5){
  tdistr <- (1 - p_0) * (-p) / log(1 - p)
  sum_distr <- p_0
  sum <- c(p_0)
  k <- 2
  
  while(tdistr > 1e-10){
    sum_distr <- sum_distr + tdistr
    sum <- c(sum, sum_distr)
    tdistr <- tdistr * p / k * (k - 1)
    k <- k + 1
  }
  
  sum
}
```

Функция моделирования одной логарифмической с нулём величины:

```{r}
rlog_0 <- function(p = 0.5, sum){
  x <- runif(1)
  j <- 1
  
  while (x > sum[j]){
    j <- j + 1
  }
  
  j - 1
}
```

Моделирование нескольких случайных величин:

```{r}
rnlog_0 <- function(n = 1, p = 0.5, sum){
  v <- replicate(n, rlog_0(p, sum))
  v
}
```

Смоделируем выборку:

```{r}
table_log <- log_0_prepering(0.75)
v <- rnlog_0(10000, 0.75, table_log)
hist(v[v < 9], probability = TRUE, breaks = seq(-0.5, max(v) + 0.5, 1), xlim = c(-0.5, 8.5), xlab = "Sample", main = "")
```

## Биномиально-логарифмическое распределение

Моделирование биномиально-логарифмического распределения:

```{r}
rbinomlog <- function(num = 1, n = 1, p = 0.5, sumlog){
  res <- c()
  
  for(i in rbinom(num, n, p)){
    res <- c(res, sum(0, rnlog(i, sumlog)))
  }
  
  res
}
```

Получение чисел Стирлинга первого рода:

```{r}
MakeNumStir <- function(data, n){
  for (i in 2:n){
    for (j in 2:n){
      if (i == j){
        data[i, j] <- 1 / gamma(i)
      } else {
        data[i, j] <- data[i - 1, j - 1] / (i - 1) + data[i - 1, j] * (i - 2) / (i - 1) 
      }
    }
  }
  
  data
}

nNumStir <- 600
NumStir <- as.data.frame(matrix(nrow = nNumStir, ncol = nNumStir))
for (i in 1:nNumStir){
  NumStir[1, i] <- 0
  NumStir[i, 1] <- 0
}
NumStir[1, 1] <- 1
NumStir <- MakeNumStir(NumStir, nNumStir)
```

Вероятности биномиально-логарифмического распределения:

```{r}
get_prob_binomlog <- function(k, p = 0.5, n = 1, q = 0.5){
  res <- 0
  alpha <- -1 / log(1 - q)
  frac <- 1

  for (j in 0:k){
    res <- res + frac * NumStir[k + 1, j + 1] * (p * alpha) ^j * (1 - p) ^(k - j)
    frac <- frac * (n - j)
  }
  
  prob <- -(1 - p) ^(n - k) * q ^k * res
  
  (1 - p) ^(n - k) * q ^k * res
}
```

Функция правдоподобия для бином-логарифма:

```{r}
m_log_lik_binomlog <- function(x.in, p = 0.5, n = 1, q = 0.5){
  if (q <= 0 || q >= 1 || p <= 0 || p >= 1 || n <= 0){
    return(-log(0));
  }
  res <- 0

  for (i in x.in){
    prob <- get_prob_binomlog(i, p, n, q)
    
    if (prob < 0 || prob > 1 || is.nan(prob))
      return(-log(0))
    
    res <- res + log(prob)
  }

  -res
}
```

## Логарифмически-биномиальное распределение

Моделирование логарифмически-биномиального распределения:

```{r}
rlogbinom <- function(num = 1, sumlog, n = 1, p = 0.5){
  res <- c()
  
  for(i in rnlog(num, sumlog)){
    res <- c(res, sum(0, rbinom(i, n, p)))
  }
  
  res
}
```

Вероятность лог-бинома для $k = 0, 1, 2, 3, 4$:

```{r}
get_prob_logbinom_logbinom_0 <- function(q = 0.5, p = 0.5, n = 1){
  log(1 - q* (1 - p) ^n) / log(1 - q)
}

get_prob_logbinom_logbinom_1 <- function(q = 0.5, p = 0.5, n = 1){
  -q * p * n * (1 - p)^(n - 1) / ((1 - q* (1 - p) ^n) * log(1 - q))
}

get_prob_logbinom_logbinom_2 <- function(q = 0.5, p = 0.5, n = 1){
  1 / 2 * (-q * p ^2 * n * (1 - p)^(n - 2) / ((1 - q* (1 - p) ^n) * log(1 - q))) *
    (q * n * (1 - p)^(n) / ((1 - q* (1 - p) ^n)) + (n - 1))
}

get_prob_logbinom_logbinom_3 <- function(q = 0.5, p = 0.5, n = 1){
  1 / 6 * (-q * p ^3 * n * (1 - p)^(n - 3) / ((1 - q* (1 - p) ^n) * log(1 - q))) *
    (2 * q ^2 * n ^2 * (1 - p)^(2 * n) / ((1 - q* (1 - p) ^n) ^2) +
      3 * q * n * (n - 1) * (1 - p)^(n) / ((1 - q* (1 - p) ^n)) + (n - 1) * (n - 2))
}

get_prob_logbinom_logbinom_4 <- function(q = 0.5, p = 0.5, n = 1){
  1 / 24 * (-q * p ^4 * n * (1 - p)^(n - 4) / ((1 - q* (1 - p) ^n) * log(1 - q))) *
    (3 * q ^3 * n ^3 * (1 - p)^(3 * n) / ((1 - q* (1 - p) ^n) ^3) +
      12 * q ^2 * n ^2 * (n - 1) * (1 - p)^(2 * n) / ((1 - q* (1 - p) ^n) ^2) +
        q * (1 - p)^(n) / ((1 - q* (1 - p) ^n)) * (3 * n * (n - 1) ^2 + 4 * n * (n - 1) * (n - 2)) +
          (n - 1) * (n - 2) * (n - 3))
}
```

Вероятности логарифмически-биномиального распределения:

```{r}
get_prob_logbinom <- function(k, q = 0.5, p = 0.5, n = 1){
  if (k == 0){
    get_prob_logbinom_logbinom_0(q, p, n)
  } else if (k == 1){
    get_prob_logbinom_logbinom_1(q, p, n)
  } else if (k == 2){
    get_prob_logbinom_logbinom_2(q, p, n)
  } else if (k == 3){
    get_prob_logbinom_logbinom_3(q, p, n)
  } else if (k == 4){
    get_prob_logbinom_logbinom_4(q, p, n)
  } else if (k >= 5){
    1 - sum(sapply(0:4, function(k) get_prob_logbinom(k, q, p, n)))
  }
}
```

Функция правдоподобия для лог-бинома:

```{r}
m_log_lik_logbinom <- function(x.in, q = 0.5, p = 0.5, n = 1){
  if (q <= 0 || q >= 1 || p <= 0 || p >= 1 || n <= 0){
    return(-log(0))
  }
  res <- 0
  
  for (i in x.in){
    prob <- get_prob_logbinom(i, q, p, n)

    if (prob < 0 || prob > 1 || is.nan(prob))
      return(-log(0))

    res <- res + log(prob)
  }
  
  -res
}
```

## Логарифмически-пуассоновское распределение

Моделирование логарифмически-пуассоновское распределения:

```{r}
rlogpois <- function(num = 1, sumlog, lambda = 1){
  res <- c()
  
  for(i in rnlog(num, sumlog)){
    res <- c(res, sum(0, rpois(i, lambda)))
  }
  
  res
}
```

```{r}
MakeNumPois <- function(data, n){
  for (i in 2:n){
    for (j in 2:n){
      if (i == j + 1){
        # data[i, j] <- 1 / gamma(i)
        data[i, j] <- 1 / gamma(i + 1)
      } else {
        # data[i, j] <- data[i - 1, j - 1] / (i - 1) + data[i - 1, j] * (i - 2) / (i - 1) 
        data[i, j] <- j * data[i - 1, j] / i + (i - j) * data[i - 1, j - 1] / i
      }
    }
  }
  
  data
}

nNumPois <- 100
NumPois <- as.data.frame(matrix(nrow = nNumPois, ncol = nNumPois))
for (i in 1:nNumPois){
  NumPois[i, 1] <- 1 / gamma(i + 1)
  NumPois[1, i] <- 0
}
NumPois[1, 1] <- 1
NumPois <- MakeNumPois(NumPois, nNumPois)
```

```{r}
get_prob_logpois <- function(k, q = 0.5, lambda = 1){
  res <- 0
  alpha <- -1 / log(1 - q)
  m <- q * exp(-lambda)
  
  if (k == 0){
    -alpha * log(1 - m)
  } else if (k == 1){
    alpha * lambda * m / (1 - m)
  } else {
    for (j in 1:(k - 1)){
      res <- res + NumPois[k, j] * m ** j
    }
    
    res * lambda ** k / (1 - m) ** k * alpha
  }
}
```

$$
P(S _\tau = k) = \frac 1 {k !} \frac {\lambda ^k} {(1 - m) ^k} \sum \limits _{j = 1} ^{k - 1} t(k, j) m ^j, \quad k = 2, ... 
$$

$$
t(k, j) = j \cdot t (k - 1, j) + (k - j) \cdot t(k - 1, j - 1)
$$

$$
\begin{aligned}
\left(\frac {\lambda ^k} {(1 - m) ^k} \sum \limits _{j = 1} ^{k - 1} t(k, j) m ^j\right)' =& \frac {km\lambda ^{k + 1}} {(1 - m) ^{k + 1}} \sum \limits _{j = 1} ^{k - 1} t(k, j) m ^j + \frac {(1 - m)\lambda ^k} {(1 - m) ^{k + 1}} \sum \limits _{j = 1} ^{k - 1} t(k, j) \lambda j m ^j =\\
=& \frac {\lambda ^{k + 1}} {(1 - m) ^{k + 1}} \left(\sum \limits _{j = 1} ^{k - 1} t(k, j) k m ^{j + 1} + \sum \limits _{j = 1} ^{k - 1} t(k, j) j m ^j - \sum \limits _{j = 1} ^{k - 1} t(k, j) j m ^{j + 1}\right) =\\
=& \frac {\lambda ^{k + 1}} {(1 - m) ^{k + 1}} \left(\sum \limits _{j = 2} ^{k} \left(t(k, j) j +t(k, j - 1) (k - j + 1)\right) m ^j + t(k, 1) m\right) =\\
=& \frac {\lambda ^{k + 1}} {(1 - m) ^{k + 1}} \left(\sum \limits _{j = 2} ^{k} t(k + 1, j) m ^j + t(k + 1, 1) m\right) =\\
=& \frac {\lambda ^{k + 1}} {(1 - m) ^{k + 1}} \sum \limits _{j = 1} ^{k} t(k + 1, j) m ^j
\end{aligned}
$$

Вероятность лог-пуассона для $k = 0, 1, 2, 3, 4$:

```{r}
get_prob_logpois_logpois_0 <- function(q = 0.5, lambda = 1){
  log(1 - q * exp(-lambda)) / log(1 - q)
}

get_prob_logpois_logpois_1 <- function(q = 0.5, lambda = 1){
  -q * lambda * exp(-lambda) / ((1 - q* exp(-lambda)) * log(1 - q))
}

get_prob_logpois_logpois_2 <- function(q = 0.5, lambda = 1){
  1 / 2 * (-q * lambda ^ 2 * exp(-lambda) / ((1 - q* exp(-lambda)) ^ 2 * log(1 - q)))
}

get_prob_logpois_logpois_3 <- function(q = 0.5, lambda = 1){
  1 / 6 * (q * exp(-lambda) + 1) * (-q * lambda ^ 3 * exp(-lambda) / ((1 - q* exp(-lambda)) ^ 3 * log(1 - q)))
}

get_prob_logpois_logpois_4 <- function(q = 0.5, lambda = 1){
  1 / 24 * (q ** 2 * exp(-2 * lambda) + 4 * q * exp(-lambda) + 1) * (-q * lambda ^ 4 * exp(-lambda) / ((1 - q* exp(-lambda)) ^ 4 * log(1 - q)))
}
```

Вероятности логарифмически-пуассоновского распределения:

```{r}
get_prob_logpois <- function(k, q = 0.5, lambda = 1){
  if (k == 0){
    get_prob_logpois_logpois_0(q, lambda)
  } else if (k == 1){
    get_prob_logpois_logpois_1(q, lambda)
  } else if (k == 2){
    get_prob_logpois_logpois_2(q, lambda)
  } else if (k == 3){
    get_prob_logpois_logpois_3(q, lambda)
  } else if (k == 4){
    get_prob_logpois_logpois_4(q, lambda)
  } else if (k >= 5){
    1 - sum(sapply(0:4, function(k) get_prob_logpois(k, q, lambda)))
  }
}
```

Гистограмма распределения:

```{r}
samLPR <- rlogpois(10000, log_prepering(0.25), lambda = 3.45)
hist.default(samLPR, breaks = seq(-0.5, max(samLPR) + 0.5, 1), probability = TRUE)
points(x = 0:max(samLPR), y = sapply(0:max(samLPR), function(k) get_prob_logpois(k, 0.25, 3.45)))
```

Функция правдоподобия для лог-бинома:

```{r}
m_log_lik_logpois <- function(x.in, q = 0.5, lambda = 1){
  if (q <= 0 || q >= 1 || lambda <= 0){
    return(-log(0))
  }
  res <- 0
  
  for (i in x.in){
    prob <- get_prob_logpois(i, q, lambda)

    if (prob < 0 || prob > 1 || is.nan(prob))
      return(-log(0))

    res <- res + log(prob)
  }
  
  -res
}
```

## Негативный бином

Функция правдоподобия негативного бинома:

```{r}
m_log_lik_nbinom <- function(x.in, q = 0.5, n = 1){
  if (q <= 0 || q >= 1 || n <= 0){
    return(-log(0));
  }
  res <- 0
  
  for (i in x.in){
    prob <- dnbinom(i, n, q)
    
    if (prob < 0 || prob > 1 || is.nan(prob))
      return(-log(0))
    
    res <- res + log(prob)
  }
  
  -res
}
```

## Свёртка негативных биномов

Моделирование свёртки отрицательно-биномиальных распределений:

```{r}
rdoublenbinom <- function(num = 1, p_1 = 0.5, size_1= 1, p_2 = 0.5, size_2 = 1){
  rnbinom(num, size_1, p_1) + rnbinom(num, size_2, p_2)
}
```

Вероятности свёртки негативных биномов:

```{r}
get_prob_doublenbinom <- function(k, p_1 = 0.5, size_1= 1, p_2 = 0.5, size_2 = 1){
  res <- 0
  
  for (m in 0:k){
    res <- res + choose(m + size_1 - 1, m) * choose(k - m + size_2 - 1, k - m) * ((1 - p_1) ** m) * ((1 - p_2) ** (k - m))
  }
  
  return(res * (p_1 ** size_1) * (p_2 ** size_2))
}
```

Функция правдоподобия свёртки негативных биномов:

```{r}
m_log_lik_doublenbinom <- function(x.in, p_1 = 0.5, size_1= 1, p_2 = 0.5, size_2 = 1){
  if (p_1 < 0 | p_1 > 1 | p_2 < 0 | p_2 > 1 | size_1 <= 0 | size_2 <= 0)
    return(-log(0))
  
  res <- 0
  
  for (i in x.in){
    prob <- get_prob_doublenbinom(i, p_1, size_1, p_2, size_2)
    
    if (prob < 0 || prob > 1 || is.nan(prob))
      return(-log(0))
    
    res <- res + log(prob)
  }
  
  -res
}
```

## Поиск оптимальных параметров и проверка согласия распределения по $\chi ^2$

Создание выборки по частотам значений случайной величины:

```{r}
generate_sample <- function(num_k){
  sam <- c()
  
  for (i in 1:length(num_k)){
    sam <- c(sam, rep.int(i - 1, num_k[i]))
  }
  
  sam
}
```

Статистика критерия $\chi ^2$:

```{r}
my_chisq <- function(exp_prob, prob){
  res <- 0
  
  for (i in 1:length(prob)){
    res <- res + (exp_prob[i] - prob[i])^2/prob[i]
  }
  
  res
}
```

Проверка гипотезы о соответствии теоритического распредления эмперическому:

```{r}
hist_make <- function (n, exp_prob, get_hist, distr = 'binomlog'){
  sam <- generate_sample(exp_prob)
  N <- length(sam)
  var_sam = var(sam) * (N - 1) / N
  mean_sam = mean(sam)
  exp_prob <- exp_prob / N
  
  if (n == 0){
    df <- -1
  }
  else{
    df <- 0
  }

  if (distr == 'binomlog'){
    if (n == 0){
      left <- 1
      right <- 1000
      while (right - left > 2) {
        nLeft <- (2 * left + right) %/% 3
        nRight <- (left + 2 * right) %/% 3
        # qLeft <- optimize(function(x) m_log_lik_binomlog(sam, p = (var_sam / mean_sam * log(1 - x) * (1 - x) - log(1 - x)) / x, n = nLeft, q = x), c(0.01, 0.99))
        # qRight <- optimize(function(x) m_log_lik_binomlog(sam, p = (var_sam / mean_sam * log(1 - x) * (1 - x) - log(1 - x)) / x, n = nRight, q = x), c(0.01, 0.99))
        qLeft <- optim(c(0.5, 1 / nLeft), function(x) m_log_lik_binomlog(sam, p = x[2], q = x[1], n = nLeft))
        qRight <- optim(c(0.5, 1 / nRight), function(x) m_log_lik_binomlog(sam, p = x[2], q = x[1], n = nRight))

        # if (qLeft$objective <= qRight$objective) {
        #   right <- nRight
        # }
        # else{
        #   left <- nLeft
        # }
        if (qLeft$value <= qRight$value) {
          right <- nRight
        }
        else{
          left <- nLeft
        }
      }
      
      # minValue <- optimize(function(x) m_log_lik_binomlog(sam, p = (var_sam / mean_sam * log(1 - x) * (1 - x) - log(1 - x)) / x, n = left, q = x), c(0.01, 0.99))
      minValue <- optim(c(0.5, 1 / left), function(x) m_log_lik_binomlog(sam, p = x[2], q = x[1], n = left))
      # q <- minValue$minimum
      q <- minValue$par[1]
      p <- minValue$par[2]
      n <- left
      for (i in (left + 1):right){
        # value <- optimize(function(x) m_log_lik_binomlog(sam, p = (var_sam / mean_sam * log(1 - x) * (1 - x) - log(1 - x)) / x, n = i, q = x), c(0.01, 0.99))
        value <- optim(c(0.5, 1 / i), function(x) m_log_lik_binomlog(sam, p = x[2], q = x[1], n = i))
        # if (value$objective <= minValue$objective){
        #   minValue <- value
        #   q <- value$minimum
        #   n <- i
        # }
        if (value$value <= minValue$value){
          minValue <- value
          q <- minValue$par[1]
          p <- minValue$par[2]
          n <- i
        }
      }
    }
    else {
      q <- optimize(function(x) m_log_lik_binomlog(sam, p = (var_sam / mean_sam * log(1 - x) * (1 - x) - log(1 - x)) / x, n = n, q = x), c(0.01, 0.99))$minimum
    }

    # res <- c(n, q, (var_sam / mean_sam * log(1 - q) * (1 - q) - log(1 - q)) / q)
    res <- c(n, q, p)
  }
  else if (distr == 'logbinom'){
    if (n == 0){
      left <- 1
      right <- 1000
      while (right - left > 1) {
        nLeft <- (2 * left + right) %/% 3
        nRight <- (left + 2 * right) %/% 3
        # qLeft <- optimize(function(x) m_log_lik_logbinom(sam, q = x, p = - log(1 - x) * (1 - x) / nLeft / x * mean_sam, n = nLeft), c(0.01, 0.99))
        # qRight <- optimize(function(x) m_log_lik_logbinom(sam, q = x, p = - log(1 - x) * (1 - x) / nRight / x * mean_sam, n = nRight), c(0.01, 0.99))
        qLeft <- optim(c(0.5, 1 / nLeft), function(x) m_log_lik_logbinom(sam, q = x[1], p = x[2], n = nLeft))
        qRight <- optim(c(0.5, 1 / nRight), function(x) m_log_lik_logbinom(sam, q = x[1], p = x[2], n = nRight))
        
        # if (qLeft$objective <= qRight$objective) {
        #   right <- nRight - 1
        # }
        # else{
        #   left <- nLeft + 1
        # }
        if (qLeft$value <= qRight$value) {
          right <- nRight - 1
        }
        else{
          left <- nLeft + 1
        }
      }
      
      # minValue <- optimize(function(x) m_log_lik_logbinom(sam, q = x, p = - log(1 - x) * (1 - x) / left / x * mean_sam, n = left), c(0.01, 0.99))
      minValue <- optim(c(0.5, 1 / left), function(x) m_log_lik_logbinom(sam, q = x[1], p = x[2], n = left))
      # q <- minValue$minimum
      q <- minValue$par[1]
      p <- minValue$par[2]
      n <- left
      print(minValue$value)
      for (i in (left + 1):right){
        # value <- optimize(function(x) m_log_lik_logbinom(sam, q = x, p = - log(1 - x) * (1 - x) / i / x * mean_sam, n = i), c(0.01, 0.99))
        value <- optim(c(0.5, 1 / i), function(x) m_log_lik_logbinom(sam, q = x[1], p = x[2], n = i))
        # if (value$objective <= minValue$objective){
        #   minValue <- value
        #   q <- value$minimum
        #   n <- i
        # }
        if (value$value <= minValue$value){
          minValue <- value
          q <- minValue$par[1]
          p <- minValue$par[2]
          n <- i
        }
      }
    }
    else {
      q <- optimize(function(x) m_log_lik_logbinom(sam, q = x, p = - log(1 - x) * (1 - x) / n / x * mean_sam, n = n), c(0.01, 0.99))$minimum
    }
    
    # res <- c(n, q, - log(1 - q) * (1 - q) / n / q * mean_sam)
    res <- c(n, q, p)
  } else if (distr == 'logpois'){
    q <- optim(c(0.5, 1), function(x) m_log_lik_logpois(sam, x[1], x[2]))
    lambda <- q$par[2]
    q <- q$par[1]
    res <- c(lambda, q)
  } else if (distr == 'nbinom'){
    q <- optim(c(0.2, 5), function(x) m_log_lik_nbinom(sam, x[1], x[2]))
    n <- q$par[2]
    q <- q$par[1]
    res <- c(n, q)
  } else if (distr == "doublenbinom"){
    q <- optim(c(0.8, 1, 0.5, 1),
               function(x) m_log_lik_doublenbinom(sam, p_1 = x[1],
                                                  size_1 = x[2],
                                                  p_2 = x[3],
                                                  size_2 = x[4]))
    res <- c(q$par[1], q$par[2], q$par[3], q$par[4])
  } else if (distr == "yulesimonnbinom"){
    q <- optim(c(0.1, 1),
               function(x) m_log_lik_yule_simon_nbinom(sam, q = x[1], n = x[2]))
    res <- c(q$par[1], q$par[2])
  }
  
  # print(res)

  prob <- c()
  
  for (i in 0:(length(exp_prob) - 2)){
    if (distr == 'binomlog'){
      prob <- c(prob, get_prob_binomlog(i, p = res[3], n = res[1], q = res[2]))
    } else if (distr == 'logbinom'){
      prob <- c(prob, get_prob_logbinom(i, q = res[2], p = res[3], n = res[1]))
    } else if (distr == 'logpois'){
      prob <- c(prob, get_prob_logpois(i, q = res[2], lambda = res[1]))
    } else if (distr == 'nbinom'){
      prob <- c(prob, dnbinom(i, size = res[1], prob = res[2]))
    } else if (distr == 'doublenbinom'){
      prob <- c(prob, get_prob_doublenbinom(i, p_1 = res[1], size_1 = res[2], p_2 = res[3], size_2 = res[4]))
    } else if (distr == "yulesimonnbinom"){
      prob <- c(prob, get_prob_yule_simon_nbinom(i, q = res[1], n = res[2]))
    }
  }
  
  prob <- c(prob, 1 - sum(prob))
  
  max_el_x = max(sam)
  max_el_y = max(exp_prob, prob)
  
  if (get_hist){
    hist.default(sam, probability = TRUE, breaks = seq(-0.5, max_el_x + 0.5, 1), xlim = c(-0.5, max_el_x + 0.5), ylim = c(0, max_el_y), xlab = "Sample", main = "")
    points(0:(length(prob) - 1), prob)
  }
  
  for (i in 1:length(prob)){
    while (100 * prob[i] < 5 && i < length(prob)){
      prob[i] <- prob[i] + prob[i + 1]
      prob <- prob[-(i + 1)]
      exp_prob[i] <- exp_prob[i] + exp_prob[i + 1]
      exp_prob <- exp_prob[-(i + 1)]
    }
  }
  
  if (100 * prob[length(prob)] < 5){
    prob[length(prob) - 1] <- prob[length(prob) - 1] + prob[length(prob)]
    prob <- prob[-length(prob)]
    exp_prob[length(exp_prob) - 1] <- exp_prob[length(exp_prob) - 1] + exp_prob[length(exp_prob)]
    exp_prob <- exp_prob[-length(exp_prob)]
  }
  
  df <- df + length(prob) - 3

  if (df < 1){
    df <- 1
  }
  
  chi <- N * my_chisq(exp_prob, prob)
  res <- c(res, 1 - pchisq(chi, df))
  
  res
}
```

### Радиобиологические данные

Функция правдоподобия для ЛБР и БЛР:

```{r}
funcMP_3D <- function(data, distr = 'binomlog', inter_par1 = c(0.01, 0.99, 100), inter_par2 = c(1, 100, 10)){
  library(rgl)
  
  open3d()
  lines3d(c(0, 0), c(0, 0), c(0, 12), color = "gray");
  lines3d(c(0, 12), c(0, 0), c(0, 0), color = "gray");
  lines3d(c(0, 0), c(0, 12), c(0, 0), color = "gray");
  
  sam <- generate_sample(data)
  N <- length(sam)
  var_sam = var(sam) * (N - 1) / N
  mean_sam = mean(sam)
  x <- c()
  y <- c()
  z <- c()
  for (par1 in seq(inter_par1[1], inter_par1[2], (inter_par1[2] - inter_par1[1]) / inter_par1[3])){
    for (par2 in seq(inter_par2[1], inter_par2[2], (inter_par2[2] - inter_par2[1]) / inter_par2[3])){
      if (distr == 'binomlog'){
        value <- m_log_lik_binomlog(sam, p = (var_sam / mean_sam * log(1 - par1) * (1 - par1) - log(1 - par1)) / par1, n = par2, q = par1)
      } else if (distr == 'logbinom'){
        value <- m_log_lik_logbinom(sam, q = par1, p = - log(1 - par1) * (1 - par1) / par2 / par1 * mean_sam, n = par2)
      } else if (distr == 'logpois'){
        value <- m_log_lik_logpois(sam, q = par1, lambda = par2)
      }
      
      if (value != 0) {
        x <- c(x, par1)
        y <- c(y, par2)
        z <- c(z, value)
      }
    }
  }
  
  z <- z - min(z)
  
  if (distr == 'binomlog'){
    z <- sqrt(z) / 10
    points3d(10 * x[z < 12], y[z < 12] * 10 / inter_par2[2], z[z < 12], color ="black")
  } else if (distr == 'logbinom'){
    points3d(10 * x[z < 12], y[z < 12] * 10 / inter_par2[2], z[z < 12], color ="black")
  } else if (distr == 'logpois'){
    points3d(10 * x[z < 12], y[z < 12] * 10 / inter_par2[2], z[z < 12], color ="black")
  }
}
```

Проверка оценки для ЛБР:

```{r warning=FALSE}
samLBR <- rlogbinom(100, log_prepering(0.4), 6, 0.3)
samLBR <- sapply(samLBR, function(x) if (x > 4){5} else {x})
histLBR <- hist(as.numeric(samLBR), probability = TRUE, breaks = seq(-0.5, max(samLBR) + 0.5, 1), xlim = c(-0.5, max(samLBR) + 0.5), plot = TRUE)
res <- hist_make(0, as.numeric(histLBR$counts), get_hist = TRUE, 'logbinom')
funcMP_3D(histLBR$counts, distr = 'logbinom')
res
```

Проверка оценки для БЛР:

```{r warning=FALSE}
samBLR <- rbinomlog(100, 6, 0.3, log_prepering(0.4))
histBLR <- hist(as.numeric(samBLR), probability = TRUE, breaks = seq(-0.5, max(samBLR) + 0.5, 1), xlim = c(-0.5, max(samBLR) + 0.5), plot = TRUE)
res <- hist_make(0, as.numeric(histBLR$counts), get_hist = TRUE, 'binomlog')
funcMP_3D(histBLR$counts)
res
```

Проверка оценки для ЛПР:

```{r warning=FALSE}
samLPR <- rlogpois(100, log_prepering(7.617121e-01), 0.4060056)
samLPR <- sapply(samLPR, function(x) if (x > 4){5} else {x})
histLPR <- hist(as.numeric(samLPR), probability = TRUE, breaks = seq(-0.5, max(samLPR) + 0.5, 1), xlim = c(-0.5, max(samLPR) + 0.5), plot = TRUE)
res <- hist_make(0, as.numeric(histLPR$counts), get_hist = TRUE, 'logpois')
funcMP_3D(histLPR$counts, distr = 'logpois', inter_par1 = c(0.01, 0.99, 100), inter_par2 = c(0.01, 2.99, 100))
res
```

```{r warning=FALSE}
samNBR <- rnbinom(100, 3, 0.4)
samNBR <- sapply(samNBR, function(x) if (x > 4){5} else {x})
histLPR <- hist(as.numeric(samNBR), probability = TRUE, breaks = seq(-0.5, max(samNBR) + 0.5, 1), xlim = c(-0.5, max(samNBR) + 0.5), plot = TRUE)
res <- hist_make(0, as.numeric(histLPR$counts), get_hist = TRUE, 'nbinom')
funcMP_3D(histLPR$counts, distr = '', inter_par1 = c(0.01, 0.99, 100), inter_par2 = c(0.01, 2.99, 100))
res
```

Загрузка радиобиологических данных:

```{r}
m <- read.csv("./VitroVivo.csv")
df.BLR <- data.frame(n = rep(0, 19), q = rep(0, 19), p = rep(0, 19), p_value = rep(0, 19))
df.LBR <- data.frame(n = rep(0, 19), q = rep(0, 19), p = rep(0, 19), p_value = rep(0, 19))
df.LPR <- data.frame(lambda = rep(0, 19), q = rep(0, 19), p_value = rep(0, 19))
df.NB <- data.frame(n = rep(0, 19), q = rep(0, 19), p_value = rep(0, 19))
```

Рассеяние:

```{r}
e.df <- data.frame(e = sapply(1:19, function(i) e_culc(generate_sample(m[i, ]))))
e.df
```

Нахождение $n, q, p$ для которых p-value максимально:

```{r warning=FALSE}
for (i in 1:19){
  # df.BLR[i, ] <- hist_make(0, as.numeric(m[i,]), get_hist = FALSE, 'binomlog')
  # df.LBR[i, ] <- hist_make(10000, as.numeric(m[i,]), get_hist = FALSE, 'logbinom')
  # df.LPR[i, ] <- hist_make(0, as.numeric(m[i,]), get_hist = FALSE, 'logpois')
  df.NB[i, ] <- hist_make(0, as.numeric(m[i,]), get_hist = FALSE, 'nbinom')
}
```

Найденные оценки и p-value:

```{r}
df.BLR
```

```{r}
df.LBR
```

```{r}
df.LPR
```

```{r}
df.NB <- df.NB |> mutate(lambda = -n / log(1 - q))
df.NB
```


Гистограммы для строки данных:

```{r warning=FALSE}
res <- hist_make(0, as.numeric(m[14,]), get_hist = TRUE, 'binomlog')
res
res <- hist_make(1000, as.numeric(m[18,]), get_hist = TRUE, 'logbinom')
res
res <- hist_make(1000, as.numeric(m[13,]), get_hist = TRUE, 'logpois')
res
```

Функция правдоподобия:

```{r warning=FALSE}
funcMP_3D(as.numeric(m[1,]), distr = 'binomlog', inter_par2 = c(1, 20, 20))
funcMP_3D(as.numeric(m[1,]), distr = 'logbinom', inter_par2 = c(1, 20, 20))
funcMP_3D(as.numeric(m[11,]), distr = 'logpois', inter_par1 = c(0.01, 0.99, 100), inter_par2 = c(0.01, 2.99, 100))
```

Проверка на стабильность модели:

```{r warning=FALSE}
plot(x = 1:50, y = seq(0, 1, length.out = 50), col = "white")
for (j in 11:19){
  n <- 1:50
  res <- c()
  for (i in n){
    if (j <= 10){
      res <- c(res, hist_make(i, as.numeric(m[j,]), get_hist = FALSE, 'binomlog')[4])
    }
    else{
      res <- c(res, hist_make(i, as.numeric(m[j,]), get_hist = FALSE, 'logbinom')[4])
    }
  }
  lines(x = n, y = res, col = j)
}
```

Корреляция с радиацией параметров и среднего:

```{r}
plot(seq(0, 45, 5), df.BLR$p[1:10], type = "l", col = "blue", ylim = c(0, max(df.BLR$p)), ylab = "Parametr p", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.BLR$p[11:19], type = "b", col = "black")

plot(seq(0, 45, 5), df.BLR$q[1:10], type = "l", col = "blue", ylim = c(0, max(df.BLR$q)), ylab = "Parametr q", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.BLR$q[11:19], type = "b", col = "black")

plot(seq(0, 45, 5), df.BLR$n[1:10], type = "l", col = "blue", ylim = c(0, max(df.BLR)), ylab = "Parametr n", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.BLR$n[11:19], type = "b", col = "black")

plot(seq(0, 45, 5), df.BLR$p[1:10] * df.BLR$n[1:10], type = "l", col = "blue", ylim = c(0, max(df.BLR$p * df.BLR$n)), ylab = "Average, n*p", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.BLR$p[11:19] * df.BLR$n[11:19], type = "b", col = "black")

mid <- c(0.38, 0.67, 0.83, 1.15, 1.70, 1.08, 1.83, 1.91, 2.23, 2.47, 0.39, 0.35, 0.59, 0.87, 0.57, 1.11, 1.13, 1.44, 1.69)

lines(seq(0, 45, 5), mid[1:10], type = "l", lty = 2, col = "green")
lines(seq(0, 40, 5), mid[11:19], type = "l", lty = 2, col = "black")
```

```{r}
plot(seq(0, 45, 5), df.LPR$q[1:10], type = "l", col = "blue", ylim = c(0, max(df.LPR$q)), ylab = "Parametr q", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.LPR$q[11:19], type = "b", col = "black")

plot(seq(0, 45, 5), df.LPR$lambda[1:10], type = "l", col = "blue", ylim = c(0, max(df.LPR$lambda)), ylab = "Parametr lambda", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.LPR$lambda[11:19], type = "b", col = "black")
```

```{r}
plot(seq(0, 45, 5), df.NB$q[1:10], type = "l", col = "blue", ylim = c(0, max(df.NB$q)), ylab = "Parametr q", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.NB$q[11:19], type = "b", col = "black")

plot(seq(0, 45, 5), df.NB$n[1:10], type = "l", col = "blue", ylim = c(0, max(df.NB$n)), ylab = "Parametr q", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), df.NB$n[11:19], type = "b", col = "black")

plot(seq(0, 45, 5), -df.NB$n[1:10] / log(1 - df.NB$q[1:10]), type = "l", col = "white", ylim = c(0, max(-df.NB$n[11:19] / log(1 - df.NB$q[11:19]))), ylab = "Parametr lambda", xlab = "Dose, Gy", main = "Black - in vitro; Blue - in vivo")
lines(seq(0, 40, 5), -df.NB$n[11:19] / log(1 - df.NB$q[11:19]), type = "b", col = "black")
```

## Анализ текстов

```{r warning=FALSE}
library(dplyr)
library(ggplot2)
library(foreach)
library(doSNOW)
library(plotly)
```

Загрузим данные:

```{r}
WordSample <- read.csv("~/All/study_materials/Диплом/МоделированиеВR/WordCutter/output.csv")

num.word <- 1000
```

### Негативный бином

Рассчитаем параметры для первых $1000$ слов:

```{r}
df.word.nbinom <- data.frame()

start_time <- Sys.time()

cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)

df.word.nbinom <- foreach(i = 1:num.word, .combine = rbind, .inorder = TRUE) %dopar% {
  histWord <- hist(as.numeric(WordSample[i, ]), probability = TRUE, breaks = seq(-0.5, max(WordSample[i, ]) + 0.5, 1), xlim = c(-0.5, max(WordSample[i, ]) + 0.5), plot = FALSE)
  res <- hist_make(0, histWord$counts, get_hist = FALSE, distr = 'nbinom')
  c(row.names(WordSample)[i], res)
}

colnames(df.word.nbinom) <- c("name", "n", "q", "p_value")
df.word.nbinom <- as.data.frame(df.word.nbinom)
df.word.nbinom <- df.word.nbinom |> mutate(n = as.numeric(n), q = as.numeric(q), p_value = as.numeric(p_value))

timediff <- difftime(Sys.time(),start_time)
cat("Расчёт занял: ", timediff, units(timediff))
```

Точечный график параметров для полученных слов:

```{r}
df.word.nbinom |> filter(n < 20) |> mutate(p_value_group = as.factor(ifelse(p_value < 0.1, 1, 2))) |>
  ggplot(aes(x = q, y = n, color = p_value_group)) +
  geom_point()
```

### Биномиально-логарифмическое распределение

Рассчитаем параметры для первых $1000$ слов:

```{r warning=FALSE}
df.word.binlog <- data.frame()

start_time <- Sys.time()

cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)

df.word.binlog <- foreach(i = 1:num.word, .combine = rbind, .inorder = TRUE) %dopar% {
  histWord <- hist(as.numeric(WordSample[i, ]), probability = TRUE, breaks = seq(-0.5, max(WordSample[i, ]) + 0.5, 1), xlim = c(-0.5, max(WordSample[i, ]) + 0.5), plot = FALSE)
  res <- hist_make(0, histWord$counts, get_hist = FALSE, distr = 'binomlog')
  c(row.names(WordSample)[i], res)
}

colnames(df.word.binlog) <- c("name", "n", "q", "p", "p_value")
df.word.binlog <- as.data.frame(df.word.binlog)
df.word.binlog <- df.word.binlog |> mutate(n = as.numeric(n), q = as.numeric(q), p = as.numeric(p), p_value = as.numeric(p_value))

timediff <- difftime(Sys.time(),start_time)
cat("Расчёт занял: ", timediff, units(timediff))
```

### Свёртка двух отрицательных биномов

Проверка оценки для свёртки отрицательных биномов:

```{r warning=FALSE}
samDNB <- rdoublenbinom(1000, 0.8, 3, 0.5, 10)
histDNB <- hist(as.numeric(samDNB), probability = TRUE, breaks = seq(-0.5, max(samDNB) + 0.5, 1), xlim = c(-0.5, max(samDNB) + 0.5), plot = TRUE)
res <- hist_make(0, as.numeric(histDNB$counts), get_hist = TRUE, 'doublenbinom')
res
```

Рассчитаем параметры для первых $1000$ слов:

```{r warning=FALSE}
df.word.doublenbinom <- data.frame()

start_time <- Sys.time()

cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)

df.word.doublenbinom <- foreach(i = 1:num.word, .combine = rbind, .inorder = TRUE) %dopar% {
  histWord <- hist(as.numeric(WordSample[i, ]), probability = TRUE, breaks = seq(-0.5, max(WordSample[i, ]) + 0.5, 1), xlim = c(-0.5, max(WordSample[i, ]) + 0.5), plot = FALSE)
  res <- hist_make(0, histWord$counts, get_hist = FALSE, distr = 'doublenbinom')
  c(row.names(WordSample)[i], res)
}

colnames(df.word.doublenbinom) <- c("name", "p1", "size1", "p2", "size2", "p_value")
df.word.doublenbinom <- as.data.frame(df.word.doublenbinom)
df.word.doublenbinom <- df.word.doublenbinom |> mutate(p1 = as.numeric(p1), size1 = as.numeric(size1), p2 = as.numeric(p2), size2 = as.numeric(size2), p_value = as.numeric(p_value))

timediff <- difftime(Sys.time(),start_time)
cat("Расчёт занял: ", timediff, units(timediff))
```

### Модификация Yule-Simon

Моделирование отрицательного бинома с параметром prob, распределённом по логарифму:

```{r}
r_yule_simon_nbinom <- function(n, ro, size){
  res <- c()
  
  for (i in rnlog(n, log_prepering(ro))){
    res <- c(res, rnbinom(1, size, exp(-i)))
  }
  
  res
}
```

Вероятности этого распределения и гистограмма:

```{r warning=FALSE}
get_prob_yule_simon_nbinom <- function(k, q = 0.5, n = 1, max_num = 1000){
  res <- 0
  
  for(i in 1:max_num){
    nb <- dnbinom(k, n, exp(-i))
    if (is.nan(nb) | nb == 0){
      break
    }
    res <- res + nb * (-q ** i / log(1 - q) / i)
  }
  
  res
}

yule_simon.q <- 0.707
yule_simon.n <- 20.99
samp.yule_simon_nbinom <- r_yule_simon_nbinom(10000, yule_simon.q, yule_simon.n)
samp.yule_simon_nbinom <- samp.yule_simon_nbinom[samp.yule_simon_nbinom < 200]
hist.default(samp.yule_simon_nbinom, breaks = seq(-0.5, max(samp.yule_simon_nbinom) + 0.5, 1), probability = TRUE)
points(x = 0:200, y = sapply(0:200, function(n) get_prob_yule_simon_nbinom(n, yule_simon.q, yule_simon.n)))
```

Функция максимального правдоподобия:

```{r}
m_log_lik_yule_simon_nbinom <- function(x.in, q = 0.5, n = 1){
  if (q <= 0 || q >= 1 || n <= 0){
    return(-log(0));
  }
  res <- 0
  
  for (i in x.in){
    prob <- get_prob_yule_simon_nbinom(i, q, n)
    
    if (prob < 0 || prob > 1 || is.nan(prob))
      return(-log(0))
    
    res <- res + log(prob)
  }
  
  -res
}
```

Рассчитаем параметры для первых $1000$ слов:

```{r}
df.word.yulesimonnbinom <- data.frame()

start_time <- Sys.time()

cl <- makeCluster(5, type = "SOCK")
registerDoSNOW(cl)

df.word.yulesimonnbinom <- foreach(i = 1:num.word, .combine = rbind, .inorder = TRUE) %dopar% {
  histWord <- hist(as.numeric(WordSample[i, ]), probability = TRUE, breaks = seq(-0.5, max(WordSample[i, ]) + 0.5, 1), xlim = c(-0.5, max(WordSample[i, ]) + 0.5), plot = FALSE)
  res <- hist_make(0, histWord$counts, get_hist = FALSE, distr = 'yulesimonnbinom')
  c(row.names(WordSample)[i], res)
}

colnames(df.word.yulesimonnbinom) <- c("name", "q", "n", "p_value")
df.word.yulesimonnbinom <- as.data.frame(df.word.yulesimonnbinom)
df.word.yulesimonnbinom <- df.word.yulesimonnbinom |> mutate(q = as.numeric(q), n = as.numeric(n), p_value = as.numeric(p_value))

timediff <- difftime(Sys.time(),start_time)
cat("Расчёт занял: ", timediff, units(timediff))
```

```{r}
df.word.yulesimonnbinom <- df.word.yulesimonnbinom |> mutate(p_value = ifelse(p_value == 1, 0, p_value))
```

### Разница между моделями

Гистограммы слова по всем расперделениям:

```{r warning=FALSE}
name <- 'man'
word <- as.numeric(WordSample[name, ])
histWord <- hist(as.numeric(word), probability = TRUE, breaks = seq(-0.5, max(word) + 0.5, 1), xlim = c(-0.5, max(word) + 0.5), main = "", xlab = "Sample")
res <- hist_make(0, histWord$counts, get_hist = TRUE, distr = 'binomlog')
res
res <- hist_make(0, histWord$counts, get_hist = TRUE, distr = 'doublenbinom')
res
res <- hist_make(0, histWord$counts, get_hist = TRUE, distr = 'nbinom')
res
# res <- hist_make(0, histWord$counts, get_hist = TRUE, distr = 'yulesimonnbinom')
# res
```

Сведём результаты в одну таблицу:

```{r}
df.word.all <- cbind(df.word.nbinom, select(df.word.binlog, -name), select(df.word.doublenbinom, -name), freq = apply(WordSample, 1, sum)[1:1000])
colnames(df.word.all) <- c("name", "n_nbinom", "q_nbinom", "pvalue_nbinom", "n_binomlog", "q_binomlog", "p_binomlog", "pvalue_binomlog", "p1_doublenbinom", "size1_doublenbinom", "p2_doublenbinom", "size2_doublenbinom", "pvalue_doublenbinom", "freq")
```

Зададим уровень значимости:

```{r}
alpha <- 0.05
```

Для каких слов все распределения хороши:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_nbinom > alpha & pvalue_binomlog > alpha & pvalue_doublenbinom > alpha)
```


Для каких слов лучше свёртка двух биномов:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_doublenbinom > alpha & pvalue_binomlog < alpha & pvalue_nbinom < alpha)
```

Для каких слов БЛР лучше:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_binomlog > alpha & pvalue_doublenbinom < alpha & pvalue_nbinom < alpha)
```

Для каких слов лучше негативный бином:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_nbinom > alpha & pvalue_doublenbinom < alpha & pvalue_binomlog < alpha)
```

Для каких слов лучше отрицательный бином и БЛР, но не свёртка:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_doublenbinom < alpha & pvalue_nbinom > alpha & pvalue_binomlog > alpha)
```

Для каких слов лучше отрицательный бином и свёртка, но не БЛР:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_doublenbinom > alpha & pvalue_nbinom > alpha & pvalue_binomlog < alpha)
```

Для каких слов лучше БЛР и свёртка, но не отрицательный бином:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_doublenbinom > alpha & pvalue_nbinom < alpha & pvalue_binomlog > alpha)
```

Для каких слов все плохи:

```{r}
df.word.all |> select(name, pvalue_nbinom, pvalue_binomlog, pvalue_doublenbinom) |> filter(pvalue_nbinom < alpha & pvalue_doublenbinom < alpha & pvalue_binomlog < alpha)
```

Отдельные классы для слов:

```{r}
df.word.all <- df.word.all |> 
  mutate(class = ifelse(pvalue_doublenbinom > alpha & pvalue_nbinom > alpha & pvalue_binomlog > alpha, "all", "-")) |> 
  mutate(class = ifelse(pvalue_doublenbinom > alpha & pvalue_nbinom < alpha & pvalue_binomlog < alpha, "doublenbinom", class)) |> 
  mutate(class = ifelse(pvalue_doublenbinom < alpha & pvalue_nbinom < alpha & pvalue_binomlog > alpha, "binomlog", class)) |> 
  mutate(class = ifelse(pvalue_doublenbinom < alpha & pvalue_nbinom > alpha & pvalue_binomlog < alpha, "nbinom", class)) |>
  mutate(class = ifelse(pvalue_doublenbinom > alpha & pvalue_nbinom > alpha & pvalue_binomlog < alpha, "nbinom+doublenbinom", class)) |>
  mutate(class = ifelse(pvalue_doublenbinom < alpha & pvalue_nbinom > alpha & pvalue_binomlog > alpha, "nbinom+binomlog", class)) |>
  mutate(class = ifelse(pvalue_doublenbinom > alpha & pvalue_nbinom < alpha & pvalue_binomlog > alpha, "doublenbinom+binomlog", class)) |>
  mutate(class = ifelse(pvalue_doublenbinom < alpha & pvalue_nbinom < alpha & pvalue_binomlog < alpha, "nothing", class))
```

Посмотрим на точечный график:

```{r}
df.word.all |> plot_ly(x = ~log(n_nbinom), y = ~q_nbinom, z = ~log(freq), color = ~class, text = ~paste('word:', name), size = I(150), alpha = 1,  width = 1200, height = 700) |>
  layout(scene = list(xaxis = list(title = 'Логарифм параметра n негативного бинома'), yaxis = list(title = 'Параметр q негативного бинома'), zaxis = list(title = 'Логарифм количества')))
```


```{r}
hist_word <- function(name, WordSample, main){
  word <- as.numeric(WordSample[name, ])
  histWord <- hist(as.numeric(word), probability = TRUE, breaks = seq(-0.5, max(word) + 0.5, 1), xlim = c(-0.5, max(word) + 0.5), main = main, xlab = name)
  return()
}
```

```{r}
df.word.all |> filter(class == 'nothing') |> apply(1, function(x) hist_word(x[1], WordSample, x[18]))
```

